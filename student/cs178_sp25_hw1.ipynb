{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs178_sp25_hw1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1> CS178: Machine Learning & Data Mining </h1>\n",
    "<h2> Homework 1: Due Friday 11 April 2025 (11:59 PM) </h2>\n",
    "<h3> Version 1.0 (Last Modified: 2 April 2025) </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "Welcome to CS 178!\n",
    "\n",
    "This homework (and many subsequent ones) will involve data analysis and reporting on methods and results\n",
    "using Python code. You will submit a **.ipynb** file, or jupyter notebook, that contains everything to Gradescope. This includes any text you wish to include to describe your results, the complete code snippets of how you attempted each problem, and any figures that were generated. It is important that you include enough detail that we know how you solved the problem, since otherwise we will be unable to grade it.\n",
    "\n",
    "\n",
    "Your homeworks will be given to you as Jupyter notebooks containing the problem descriptions and some template code that will help you get started. You should not modify the function signatures, only the code inside the function definitions. You may add additional cells (containing either code, helper functions, or text) as needed. This will help ensure that all of the code for the solutions is included. Do not delete any cells pertaining to the Autograder to ensure that you can receive credit for your work.\n",
    "\n",
    "Several problems in this assignment require you to create plots. Use `matplotlib.pyplot` to do this, which is already imported for you as `plt`. Do not use any other plotting libraries, such as `seaborn`. Unless you are told otherwise, you should call `pyplot` plotting functions with their default arguments.\n",
    "\n",
    "If you have any questions/concerns about using Jupyter notebooks, ask us on EdDiscussion.\n",
    "\n",
    "### Summary of Assignment: 100 total points\n",
    "- [Problem 1: Exploring the Abalone Dataset (31 points)](#problem-1-exploring-the-abalone-dataset)\n",
    "  - [Problem 1.1: Numpy Arrays (3 points)](#problem-11-3-points-numpy-arrays)\n",
    "    - [1.1.a: Abalone Shape (1 point)](#problem-11a-abalone-shape-1-point)\n",
    "    - [1.1.b: Abalone Features (1 point)](#problem-11b-abalone-features-1-point)\n",
    "    - [1.1.c: Abalone Slicing (1 point)](#problem-11c-abalone-slicing-1-point)\n",
    "  - [Problem 1.2: Feature Statistics (4 points)](#problem-12-4-points-feature-statistics)\n",
    "  - [Problem 1.3: Introduction to Pandas (10 points)](#problem-13-9-points-introduction-to-pandas)\n",
    "    - [1.3.a: Abalone Shape (Pandas) (1 point)](#problem-13a-abalone-features-pandas)\n",
    "    - [1.3.b: Abalone Features & Datatypes (Pandas) (1 point)](#problem-13b-abalone-features--datatypes-pandas)\n",
    "    - [1.3.c: Abalone DataFrame Slicing Slicing (Pandas) (1 point)](#problem-13c-abalone-dataframe-slicing-pandas)\n",
    "    - [1.3.d: Abalone DataFrame Subsetting (Pandas) (1 point)](#problem-13d-abalone-dataframe-subsetting-pandas)\n",
    "    - [1.3.e: Abalone NaN Removal (Pandas)(1 point)](#problem-13e-abalone-nan-removal-pandas)\n",
    "    - [1.3.f: Abalone Summary (Pandas) (1 point)](#q13f-abalone-summary)\n",
    "    - [1.3.g: Abalone Sample Statistics (Pandas) (1 point)](#problem-13g-abalone-sample-statistics-pandas)\n",
    "    - [1.3.h: Object to Categorical (2 points)](#problem-13h-object-to-categorical)\n",
    "    - [1.3.i: Abalone Unique Classes (1 point)](#problem-13i-abalone-unique-classes)\n",
    "  - [Problem 1.4: Plotting Features with matplotlib (4 points)](#problem-14-plotting-features-with-matplotlib)\n",
    "    - [1.4.a: Plotting Feature Histograms with matplotlib (2 points)](#problem-14a-2-points-plotting-feature-histograms-with-matplotlib)\n",
    "    - [1.4.b: Plotting Distributions per Category with matplotlib (2 points)](#problem-14b-2-points-plotting-distributions-per-category-with-matplotlib)\n",
    "  - [Problem 1.5: Scatter Plots (10 points)](#problem-15-10-points-feature-scatter-plots)\n",
    "- [Problem 2: Nearest Centroid Classifiers on Abalone Dataset (35 points)](#problem-2-nearest-centroid-classifiers)\n",
    "  - [Problem 2.1: Unown MNIST Visualization (5)](#problem-21-5-points-unown-mnist-visualization)\n",
    "  - [Problem 2.2: Implementing Nearest Centroids Classifier (20)](#problem-22-20-points-implementing-a-nearest-centroid-classifier)\n",
    "  - [Problem 2.3: Evaluating Nearest Centroids Classifier (10)](#problem-23-10-points-evaluating-nearest-centroids-classifier)\n",
    "- [Problem 3: kNN Classifiers on Abalone Dataset (29 points)](#problem-3-knn)\n",
    "  - [Problem 3.1: Decision Boundaries (15 points)](#problem-31-decision-boundaries)\n",
    "    - [Problem 3.1.a: Train/Test Split (4 points)](#problem-31a-4-points-train/test-split)\n",
    "    - [Problem 3.1.b: kNN Decision Boundary Plot (10 points)](#problem-31b-10-points-knn-decision-boundary-plot)\n",
    "  - [Problem 3.2: Plot the Error Rate for Various k (15 points)](#problem-32-15-points-error-rates-vs-k)\n",
    "- [Statement of Collaboration (5 points)](#statement-of-collaboration-4-points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let's import some libraries that you will make use of in this assignment. Make sure that you run the code cell below in order to import these libraries.\n",
    "\n",
    "**Important: In the code block below, we set `hw1_seed=1234`. This is to ensure your code has reproducible results and is important for grading. Do not change this. If you are not using the provided Jupyter notebook, make sure to also set the random seed as below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "hw1_seed = 1234\n",
    "np.random.seed(hw1_seed)  \n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Exploring the Abalone Dataset\n",
    "In this problem, you will explore some basic data manipulation and visualizations with the abalone dataset. The dataset consists of physical measurements of male (M), female (F), and infant (I) abalone molluscs. For every datapoint, we are given several real-valued features which will be used to predict what sex of abalone it is. Let's first load in the dataset by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "# Load in the abalone dataset\n",
    "abalone_file = \"data/abalone_edited.csv\"\n",
    "abalone_array = np.genfromtxt(abalone_file, names=True, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Problem 1.1 (3 points): Numpy Arrays\n",
    "\n",
    "The variable `abalone_array` is a numpy array containing the feature vectors in our dataset, and the corresponding labels.\n",
    "\n",
    "- What is the shape of `abalone_array`? ([Hint](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html)) (1.1a)\n",
    "- How many features are in the dataset? (1.1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 1.1.a: Abalone Shape (1 point)\n",
    "Write a function that takes in the numpy dataset, and returns the shape as a tuple `(num_rows, num_columns)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.1a Abalone Shape"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4183,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def abalone_shape(dataset: np.ndarray) -> Tuple[int, int]:\n",
    "    return np.shape(dataset)\n",
    "abalone_shape(abalone_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 1.1.b: Abalone Features (1 point)\n",
    "Write a function that takes in the numpy dataset, and returns the features as a list of strings.\n",
    "\n",
    "As an exercise for yourself, consider:\n",
    "- How many datapoints are in our dataset, and how many features does each datapoint have?\n",
    "- How many different classes (i.e. labels)  are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.1b Abalone Features"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex',\n",
       " 'Length',\n",
       " 'Diameter',\n",
       " 'Height',\n",
       " 'Whole_weight',\n",
       " 'Shucked_weight',\n",
       " 'Viscera_weight',\n",
       " 'Shell_weight',\n",
       " 'Rings']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_abalone_features(dataset: np.ndarray) -> List[str]:\n",
    "    return list(dataset.dtype.names)\n",
    "\n",
    "get_abalone_features(abalone_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 1.1.c: Abalone Slicing (1 point)\n",
    "In 1.1.c you will implement a function `select_feature_range` where given a `feature` column name and a `start_row`, and `end_row`, return the selected rows and the selected feature as a numpy array. Answer the first questions to help guide you to an implementation for `select_feature_range`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(nan, 0.455, 0.365, 0.095, 0.514 , 0.2245, 0.101 , 0.15 , 15.),\n",
       "       (nan, 0.35 , 0.265, 0.09 , 0.2255, 0.0995, 0.0485, 0.07 ,  7.),\n",
       "       (nan, 0.53 , 0.42 , 0.135, 0.677 , 0.2565, 0.1415, 0.21 ,  9.),\n",
       "       (nan, 0.44 , 0.365, 0.125, 0.516 , 0.2155, 0.114 , 0.155, 10.),\n",
       "       (nan, 0.33 , 0.255, 0.08 , 0.205 , 0.0895, 0.0395, 0.055,  7.)],\n",
       "      dtype=[('Sex', '<f8'), ('Length', '<f8'), ('Diameter', '<f8'), ('Height', '<f8'), ('Whole_weight', '<f8'), ('Shucked_weight', '<f8'), ('Viscera_weight', '<f8'), ('Shell_weight', '<f8'), ('Rings', '<f8')])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array indexing and slicing\n",
    "# extract the first 5 rows of the dataset\n",
    "def extract_first_five(dataset: np.ndarray) -> np.ndarray:\n",
    "    return dataset[:5]\n",
    "    \n",
    "extract_first_five(abalone_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.514 , 0.2255, 0.677 , ..., 1.176 , 1.0945, 1.9485], shape=(4183,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the column Whole_weight from the dataset\n",
    "def extract_whole_weight(dataset: np.ndarray):\n",
    "    return dataset['Whole_weight']\n",
    "\n",
    "extract_whole_weight(abalone_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2255, 0.677 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract from index 1 to 3 of the column Whole_weight\n",
    "def extract_whole_weight(dataset: np.ndarray, startIdx: int, endIdx: int):\n",
    "    return dataset['Whole_weight'][startIdx : endIdx]\n",
    "\n",
    "extract_whole_weight(abalone_array, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def select_feature_range(\n",
    "    dataset: np.ndarray, \n",
    "    feature: str, \n",
    "    start: int, \n",
    "    end: int) -> np.ndarray:\n",
    "    \n",
    "    return dataset[feature][start : end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.1c Abalone Slicing"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2255, 0.677 , 0.516 ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_feature_range(abalone_array, 'Whole_weight', 1, 4)   # HINT: Same as abalone_array['Whole_weight'][1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 1.2 (4 points): Feature Statistics\n",
    "\n",
    "Let's compute some statistics about our features. You are allowed to use `numpy` to help you with this problem -- for example, you might find some of the `numpy` functions listed [here](https://numpy.org/doc/stable/reference/routines.statistics.html) or [here](https://numpy.org/doc/stable/reference/routines.math.html) useful.\n",
    "\n",
    "- Compute the mean and standard deviation for an arbitrary column.\n",
    "- Compute the minimum and maximum value for an arbitrary column.\n",
    "\n",
    "Make sure to print out each of these values, and indicate clearly which value corresponds to which computation.\n",
    "\n",
    "Hint: Numpy has functions that will help you with this. Make sure to use the ones that deal with nan values since some features may have missing data. We will look at other ways to deal with missing data in subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5239846926572591)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mean(dataset, column_name) :\n",
    "    return np.nanmean(dataset[column_name])\n",
    "#get_mean(abalone_array, 'Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.12003259843872688)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_std_dev(dataset, column_name) :\n",
    "    return np.nanstd(dataset[column_name])\n",
    "#get_std_dev(abalone_array, 'Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.075)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_min(dataset, column_name) :\n",
    "    return np.nanmin(dataset[column_name])\n",
    "#get_min(abalone_array, 'Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.2 Feature Statistics"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.815)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max(dataset, column_name) :\n",
    "    return np.nanmax(dataset[column_name])\n",
    "#get_max(abalone_array, 'Length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3 (9 Points): Introduction to Pandas\n",
    "\n",
    "In this problem, you will be reimplementing 1.1 and 1.2, but using Pandas instead of Numpy.\n",
    "\n",
    "Transitioning from using NumPy arrays to Pandas for data manipulation involves understanding the core differences between the two libraries and how to leverage Pandas' capabilities to work with data more efficiently.\n",
    "\n",
    "NumPy arrays require homogeneous data types. This homogeneity is beneficial for mathematical operations because it allows for efficient memory usage and optimized computation.\n",
    "\n",
    "On the other hand, Pandas DataFrames are designed to work seamlessly with heterogeneous data. This flexibility makes DataFrames ideal for real-world datasets, such as those found in CSV files, Excel spreadsheets, and SQL databases. Pandas offers a wide range of functions for data manipulation, such as handling missing data, performing operations on columns and rows, and transforming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.a Abalone Features (Pandas)\n",
    "\n",
    "To compare using Pandas and Numpy, use the feature list to fetch the associated column index number that matches what is known as the \"feature\" in Pandas, `col_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell\n",
    "abalone_df = pd.read_csv(abalone_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.a Abalone Features (Pandas)"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n",
      " Index(['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',\n",
      "       'Viscera weight', 'Shell weight', 'Rings'],\n",
      "      dtype='object')\n",
      "Numpy:\n",
      " ['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Rings']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get column names from a Pandas DataFrame (df):\n",
    "print(\"Pandas:\\n\", abalone_df.columns)\n",
    "# to get column names from a Numpy array (np):\n",
    "abalone_col_names_np = get_abalone_features(abalone_array)\n",
    "print(\"Numpy:\\n\", abalone_col_names_np)\n",
    "\n",
    "# find column index of key given\n",
    "def get_col_idx(col_name: str, feature_list: List[str]) -> int:\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    for i, item in enumerate(feature_list):\n",
    "        if col_name == item:\n",
    "            return i\n",
    "    return -1\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "get_col_idx('Shucked weight',abalone_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.b Abalone Features & Datatypes (Pandas)\n",
    "\n",
    "Now, let's access feature names by column. Since we are using Pandas, we can access the column using dictionary syntax, where the feature is the key, and the values are a column (containing entries from every row).\n",
    "\n",
    "We will explore different ways of isolating rows and columns in Pandas in the following problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.b Abalone Features by Column (Pandas)"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_3b_1 = 0       0.1500\n",
      "1       0.0700\n",
      "2       0.2100\n",
      "3       0.1550\n",
      "4       0.0550\n",
      "         ...  \n",
      "4178    0.2490\n",
      "4179    0.2605\n",
      "4180    0.3080\n",
      "4181    0.2960\n",
      "4182    0.4950\n",
      "Name: Shell weight, Length: 4183, dtype: float64 \n",
      " q1_3b_2 = <class 'pandas.core.series.Series'> \n",
      " q1_3b_3 = <class 'numpy.float64'> \n",
      " q1_3b_4 = (0       0.1500\n",
      "1       0.0700\n",
      "2       0.2100\n",
      "3       0.1550\n",
      "4       0.0550\n",
      "         ...  \n",
      "4178    0.2490\n",
      "4179    0.2605\n",
      "4180    0.3080\n",
      "4181    0.2960\n",
      "4182    0.4950\n",
      "Name: Shell weight, Length: 4183, dtype: float64, 0       15.0\n",
      "1        7.0\n",
      "2        9.0\n",
      "3       10.0\n",
      "4        7.0\n",
      "        ... \n",
      "4178    11.0\n",
      "4179    10.0\n",
      "4180     9.0\n",
      "4181    10.0\n",
      "4182    12.0\n",
      "Name: Rings, Length: 4183, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# select col from df\n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# extract the column Shell weight from the dataset\n",
    "q1_3b_1 = abalone_df['Shell weight']\n",
    "\n",
    "# get datatype of the column Shell weight\n",
    "q1_3b_2 = type(abalone_df['Shell weight'])\n",
    "\n",
    "# get datatype of an entry in the column Shell weight\n",
    "q1_3b_3 = type(abalone_df[\"Shell weight\"].iloc[0])\n",
    "\n",
    "# extract the columns Shell weight and Rings\n",
    "q1_3b_4 = abalone_df['Shell weight'], abalone_df['Rings']\n",
    "\n",
    "### YOUR CODE ENDS HERE ###\n",
    "\n",
    "print(f\"q1_3b_1 = {q1_3b_1} \\n q1_3b_2 = {q1_3b_2} \\n q1_3b_3 = {q1_3b_3} \\n q1_3b_4 = {q1_3b_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.c Abalone DataFrame Slicing (Pandas)\n",
    "\n",
    "The distinction between `.loc` and `.iloc` [indexing in Pandas](https://pandas.pydata.org/docs/user_guide/indexing.html) is crucial for efficient data manipulation and analysis. Understanding the differences between these two methods is essential for selecting specific rows and columns from a DataFrame based on either labels or integer positions.\n",
    "\n",
    "`.loc` is primarily label-based indexing, which means you have to specify rows and columns based on their labels. This method is very powerful because it allows for more complex indexing and selection of data. You can use boolean masks, slices, and even callable functions to select data.\n",
    "\n",
    "`.iloc`, on the other hand, is purely integer-based indexing. This method is useful when you need to access data by its position in the DataFrame, regardless of the labels.\n",
    "\n",
    "Be sure your variable names match the ones listed in the print statement exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.c Abalone DataFrame Slicing (Pandas)"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# loc vs iloc indexing\n",
    "\n",
    "# extract the column Length from the dataset using loc\n",
    "q1_3c_1 = ...\n",
    "\n",
    "# extract the column Length from the dataset using iloc and the function you wrote before get_col_idx\n",
    "q1_3c_2 = ...\n",
    "\n",
    "# get all rows where the Sex is 'I'\n",
    "q1_3c_3 = ...\n",
    "\n",
    "# get all rows where the Length is between 0.4 and 0.9\n",
    "q1_3c_4 = ...\n",
    "\n",
    "print(f\"q1_3c_1 = {q1_3c_1} \\n q1_3c_2 = {q1_3c_2} \\n q1_3c_3 = {q1_3c_3} \\n q1_3c_4 = {q1_3c_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.d Abalone DataFrame Subsetting (Pandas)\n",
    "\n",
    "Taking the learnings from the previous questions, create a new DataFrame that is a subset of the original abalone dataframe, containing the features in `abalone_attributes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "abalone_attributes = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Rings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.d Abalone DataFrame Subsetting (Pandas)"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_subset_df_by_attribute(attributes : List[str], df : pd.DataFrame) -> pd.DataFrame :\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.e Abalone NaN Removal (Pandas)\n",
    "\n",
    "NaN values represent missing or undefined data. In many cases, these missing values can disrupt the flow of data analysis and computations.\n",
    "\n",
    "Pandas provides several methods for removing NaN values, including `dropna()`, `replace()`, and `interpolate()`. The `dropna()` method removes rows or columns with NaN values, `replace()` allows you to replace NaN values with a specified value, and `interpolate()` fills NaN values with interpolated values. Each of these methods serves different needs depending on the context and the desired outcome of your data cleaning process.\n",
    "\n",
    "This problem will focus on using `dropna()`.\n",
    "\n",
    "- NOTE: After cleaning, `abalone_clean` will become the default DataFrame we work with for the remainder of the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.e Abalone NaN Removal (Pandas)"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_nan(dataset : pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Remove individual samples that have NaN values\n",
    "    '''\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Q1.3.f Abalone Summary\n",
    "\n",
    "Pandas provides useful methods to quickly generate key statistics over the entire dataframe. To get the description of the dataset, use the `describe()` member function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.f Abalone Summary"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_description(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "abalone_sub_df = create_subset_df_by_attribute(abalone_attributes, abalone_df)\n",
    "abalone_clean = remove_nan(abalone_sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.g Abalone Sample Statistics (Pandas)\n",
    "\n",
    "Use existing methods from Pandas to calculate the mean (`mean`), standard deviation (`std`), minimum value (`min_val`), and maximum value (`max_val`). Make sure the variable names match the ones in the print statement exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.g Abalone Sample Statistics"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Find stats for Diameter column\n",
    "\n",
    "# Mean\n",
    "mean = ...\n",
    "mean\n",
    "\n",
    "# STD\n",
    "std = ...\n",
    "std\n",
    "\n",
    "# Min & Max values\n",
    "min_val = ...\n",
    "min_val\n",
    "max_val = ...\n",
    "max_val\n",
    "\n",
    "print(f\"Mean: \\n{mean}\\n\\nStandard Deviation: \\n{std}\\n\\nMin Val: \\n{min_val}\\n\\nMax Val: \\n{max_val}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.h Object to Categorical\n",
    "\n",
    "When a homogenous datatype for a column cannot be deduced, it becomes an object type by default. In Pandas, Categorical data is a datatype that accepts a finite set of possible values. We would like to convert `Sex` and `Rings` from the default `object` to `categorical` and `int64` respectively for the `abalone_clean` DataFrame.\n",
    "\n",
    "[Useful link for more on `categorical`](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# convert Sex column to type category\n",
    "abalone_clean[\"Sex\"] = ...\n",
    "# convert Rings column to type int64\n",
    "abalone_clean['Rings'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "otter": {
     "tests": [
      "Q1.3.h Object to Categorical"
     ]
    }
   },
   "outputs": [],
   "source": [
    "abalone_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Problem 1.3.i Abalone Unique Classes\n",
    "\n",
    "Use the `unique()` member function to get the unique class labels for the `Sex`. Be sure to assign it to the same variable name shown in the print statement to get points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "Q1.3.i Abalone Unique Classes"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Print different classes of Abalone's sex\n",
    "q1_3i = ...\n",
    "q1_3i\n",
    "print(f\"q1_3i = {q1_3i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 1.4: Plotting Features with Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Problem 1.4.a (2 Points): Plotting Feature Histograms with matplotlib\n",
    "Now, you will visualize the distribution of each feature with histograms.  Use `matplotlib.pyplot` to do this, which is already imported for you as `plt`. Do not use any other plotting libraries, such as `seaborn`.\n",
    "\n",
    "- For every feature in `abalone_clean`, plot a histogram of the values of the feature. Your plot should consist of a grid of subplots with 1 row and 6 columns.\n",
    "- Include a title above each subplot to indicate which feature we are plotting. For example, if the first feature is \"Rings\" you can call the first feature \"Rings\", the second feature \"Whole weight\", etc.\n",
    "\n",
    "Some starter code is provided for you below. (Hint: `axes[0].hist(...)` will create a histogram in the first subplot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a figure with 1 row, and 6 columns\n",
    "figure, axes = plt.subplots(1, 6, figsize=(12, 3))  \n",
    "\n",
    "# Select physical features of the abalone (Sex, Length, Diameter, Height, Whole weight, Rings)\n",
    "abalone_phys_attr = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Rings\"]\n",
    "\n",
    "# Plot histogram for each feature\n",
    "# Include a title on each subplot\n",
    "### YOUR CODE STARTS HERE ###\n",
    "...\n",
    "### YOUR CODE ENDS HERE ###\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 1.4.b (2 Points): Plotting Distributions per Category with matplotlib\n",
    "Now, you will visualize the distribution of \"Whole weight\" and \"Length\" with Violin plots.  Use `matplotlib.pyplot` to do this, which is already imported for you as `plt`. Do not use any other plotting libraries, such as `seaborn`.\n",
    "\n",
    "- For every class in `abalone_clean[\"Sex\"]`, plot a Violin plot of the values of \"Whole weight\" and \"Length\". Your plot should consist of a grid of subplots with 1 row and 2 columns.\n",
    "- Include a title above each subplot to indicate which feature we are plotting. \n",
    "\n",
    "Some starter code is provided for you below. (Hint: `axes[0].violinplot(...)` will create a violinplot in the first subplot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# Plot a scatter for each feature pair.\n",
    "# Make sure to color the points by their class label.\n",
    "# Include an x-label and a y-label for each subplot.\n",
    "...\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 1.5 (10 points): Feature Scatter Plots\n",
    "To help further visualize the abalone datset, you will now create several scatter plots of the features. Use `matplotlib.pyplot` to do this, which is already imported for you as `plt`. Do not use any other plotting libraries, such as `seaborn`.\n",
    "\n",
    "- For every pair of features in `abalone_X`, plot a scatter plot of the feature values, colored according to their labels. For example, plot all data points with `F` to red, `M` to blue, `I` to orange.\n",
    "- Include an x-label and a y-label on each subplot to indicate which features we are plotting.\n",
    "\n",
    "Use the feature as the label for the x-axis and y-axis.\n",
    "\n",
    "Some starter code is provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# create a figure  with 6 rows and 6 columns for features in abalone_clean\n",
    "figure, axes = plt.subplots(6, 6, figsize=(12, 12))\n",
    "\n",
    "# Define the features to plot\n",
    "abalone_X = abalone_clean[abalone_phys_attr]\n",
    "\n",
    "# Create a dictionary mapping sex to unique colors\n",
    "sex_to_color = {'F': 'red', 'M': 'blue', 'I': 'orange'}\n",
    "\n",
    "# Replace sex letter (M,F,I) with integers in the DataFrame\n",
    "abalone_color = abalone_clean['Sex'].map(sex_to_color)\n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# Plot a scatter for each feature pair.\n",
    "# Make sure to color the points by their class label.\n",
    "# Include an x-label and a y-label for each subplot.\n",
    "...\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Problem 2: Nearest Centroid Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will implement a nearest centroid classifier and train it on the Unown-MNIST dataset. Unown-MNIST is divided into a training set of 25,000 unown pokemon. Each sample is a grayscale image sized 28x28 pixels, paired with one of 10 class labels. Run the following block of code to load the Unown-MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features and labels for the MNIST dataset\n",
    "unown_X, unown_y = np.load(\"data/unown_X.npy\"), np.load(\"data/unown_y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, we split the Unown-MNIST dataset into training and testing sets -- 75% of the data is used for training, and 25% is used for testing. The function `train_test_split` is provided by scikit-learn, and will automatically shuffle our data for us if we use the flag `shuffle=True`. \n",
    "\n",
    "**NOTE:** For this homework, do not alter the flag `random_state=hw1_seed`, as this is necessary for obtaining reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unown_X_tr, unown_X_te, unown_y_tr, unown_y_te = train_test_split(unown_X, unown_y, \n",
    "                                                                  test_size=0.25, random_state=hw1_seed, shuffle=True)\n",
    "unown_X_tr\n",
    "unown_X_te\n",
    "unown_y_tr\n",
    "unown_y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 2.1 (5 points): Unown-MNIST Visualization\n",
    "Let's begin by visualizing a few of the images in the Unown-MNIST dataset. Use `matplotlib.pyplot` to do this, which is already imported for you as `plt`. Do not use any other plotting libraries, such as `seaborn`.\n",
    "\n",
    "- Plot the first 16 images in `unown_X_tr` in a 4x4 grid. ([Hint 1](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) [Hint 2](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html))\n",
    "- Include a title for each subplot indicating the label of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Some default settings for our plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create a figure with 4 rows and 4 columns\n",
    "figure, axes = plt.subplots(4, 4, figsize=(6, 6))  \n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# Plot the first 16 images in our dataset.\n",
    "# Include a title on each subplot to indicate the corresponding label.\n",
    "# (â‰ˆ 5 lines of code)\n",
    "\n",
    "...\n",
    "\n",
    "### YOUR CODE ENDS HERE  ###\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 2.2 (20 points): Implementing a Nearest Centroid Classifier\n",
    "\n",
    "In the code given below, we define the class `NearestCentroidClassifier` which has an unfinished implementation of a nearest centroid classifier. For this problem, you will complete this implementation. Your nearest centroid classifier will use the Euclidean distance, which is defined for two feature vectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$ as\n",
    "\n",
    "$$d_E(\\mathbf{x}_1, \\mathbf{x}_2) = \\sqrt{\\sum_{j=1}^d (x_{1j} - x_{2j})^2}. $$\n",
    "\n",
    "\n",
    "- Implement the method `fit`, which takes in an array of features `X` and an array of labels `y` and trains our classifier.  You should store your computed centroids in the list `self.centroids`.\n",
    "- Test your implementation of `fit` by training a `NearestCentroidClassifier` on the Unown-MNIST training set, and using the provided method `plot_centroids` to visualize the centroids. If your implementation is correct, the centroids should resemble the corresponding class label in the plot.\n",
    "- Implement the method `predict`, which takes in an (array of) feature vectors `X` and predicts their class labels.\n",
    "- Print the predicted labels (using your `predict` function) and the true labels for the first ten images in the Unown-MNIST testing set. Make sure to indicate which are the predicted labels and which are the true labels.\n",
    "\n",
    "You are allowed to modify the given code as necessary to complete the problem, e.g. you may create helper functions. Leave the type hints and function signatures as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class NearestCentroidClassifier:\n",
    "    def __init__(self):\n",
    "        # A list containing the centroids; to be filled in with the fit method.\n",
    "        self.centroids = []  \n",
    "        \n",
    "    def plot_centroids(self):\n",
    "        # Some default settings for our plots\n",
    "        plt.rcParams['image.interpolation'] = 'nearest'\n",
    "        plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "        # Create a figure with 2 rows and 5 columns\n",
    "        figure, axes = plt.subplots(2, 5, figsize=(12, 4))  \n",
    "        \n",
    "        # Plot the centroids\n",
    "        for i in range(10):\n",
    "            axes[i//5, i%5].imshow(self.centroids[i].reshape(28, 28))\n",
    "            axes[i//5, i%5].set_title(f'Label: {i}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" \n",
    "        Fits the nearest centroid classifier with training features X and training labels y.\n",
    "        \n",
    "        Parameters:\n",
    "        X (np.ndarray): array of training features; shape (n, d), where n is the number of datapoints, and d is the number of features.\n",
    "        y (np.ndarray): array training labels; shape (n, ), where n is the number of datapoints.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        # Hint: you should append to self.centroids with the corresponding centroids.\n",
    "        \n",
    "        ...\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "                    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" \n",
    "        Makes predictions with the nearest centroid classifier on the features in X.\n",
    "\n",
    "        Parameters:\n",
    "        X (np.ndarray): array of features; shape (n, d), where n is the number of datapoints, and d is the number of features.\n",
    "\n",
    "        Returns:\n",
    "        y_pred (np.ndarray): a numpy array of predicted labels; shape (n, ), where n is the number of datapoints.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "        ...\n",
    "        ### YOUR CODE ENDS HERE ###    \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Here is some code illustrating how to use your `NearestCentroidClassifier`. You can run this code to fit your classifier and to plot the centroids. You should write your implementation above such that you don't need to modify the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_classifier = NearestCentroidClassifier()\n",
    "nc_classifier.fit(unown_X_tr, unown_y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_classifier.plot_centroids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predicted and true labels for the first ten images in the Unown-MNIST testing set\n",
    "y_hat_te = nc_classifier.predict(unown_X_te[:10, :])\n",
    "print(\"Predicted labels:\", y_hat_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True labels:\", unown_y_te[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.3 (10 points): Evaluating Nearest Centroids Classifier\n",
    "\n",
    "Now that you've implemented the nearest centroid classifier, it is time to evaluate its performance.\n",
    "\n",
    "- Write a function `compute_accuracy` that computes the accuracy of a model's predictions. That is, your function should take in an array of true labels y and an array of predicted labels `y_pred`, and return the accuracy of the predictions. You may use numpy to do this, but do not use `sklearn` or any other machine learning libraries.\n",
    "- Write a function that computes the confusion matrix of a model's predictions. That is, your function should  take in an array of true labels `y`and an array of predicted labels `y_pred`, and return corresponding $C \\times C$ confusion matrix as a numpy array, where $C$ is the number of classes. You may use numpy to do this, but do not use `sklearn` or any other machine learning libraries.\n",
    "- Verify that your implementations of `NearestCentroidClassifier`, `compute_accuracy`, and `compute_confusion_matrix` are correct. To help you do this, you are given the functions `eval_sklearn_implementation` and `eval_my_implementation`. The function `eval_sklearn_implementation` will use the relevant `sklearn` implementations to compute the accuracy and confusion matrix of a nearest centroid classifier. The function `eval_my_implementation` will do the same, but for your implementations. If your code is correct, the outputs of the two functions should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y, y_pred):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    accuracy = ...\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y, y_pred):\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    ...\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "### Results with the sklearn implementation ###\n",
    "###############################################\n",
    "\n",
    "def eval_sklearn_implementation(X_tr, y_tr, X_te, y_te):\n",
    "    # Nearest centroid classifier implemented in sklearn\n",
    "    sklearn_nearest_centroid = NearestCentroid()\n",
    "\n",
    "    # Fit on training dataset\n",
    "    sklearn_nearest_centroid.fit(X_tr, y_tr)\n",
    "\n",
    "    # Make predictions on training and testing data\n",
    "    sklearn_y_pred_tr = sklearn_nearest_centroid.predict(X_tr)\n",
    "    sklearn_y_pred_te = sklearn_nearest_centroid.predict(X_te)\n",
    "\n",
    "    # Evaluate accuracies using the sklearn function accuracy_score\n",
    "    sklearn_acc_tr = accuracy_score(y_tr, sklearn_y_pred_tr)\n",
    "    sklearn_acc_te = accuracy_score(y_te, sklearn_y_pred_te)\n",
    "\n",
    "    print(f'Sklearn Results:')\n",
    "    print(f'--- Accuracy (train): {sklearn_acc_tr}')\n",
    "    print(f'--- Accuracy (test): {sklearn_acc_te}')\n",
    "\n",
    "    # Evaluate confusion matrix using the sklearn function confusion_matrix\n",
    "    sklearn_cm = confusion_matrix(y_te, sklearn_y_pred_te)\n",
    "    sklearn_disp = ConfusionMatrixDisplay(confusion_matrix = sklearn_cm)\n",
    "    sklearn_disp.plot()\n",
    "    \n",
    "# Call the function    \n",
    "unown_X_tr_flattened = unown_X_tr.reshape((18750, 784))\n",
    "unown_X_te_flattened = unown_X_te.reshape((6250, 784))\n",
    "\n",
    "non_zero_var = []\n",
    "\n",
    "for i in range(784):\n",
    "    unique = np.unique(unown_X_tr_flattened[:, i])\n",
    "    if len(unique) != 1:\n",
    "        non_zero_var.append(i)\n",
    "\n",
    "print(len(non_zero_var))\n",
    "\n",
    "eval_sklearn_implementation(unown_X_tr_flattened[:, non_zero_var], unown_y_tr, unown_X_te_flattened[:, non_zero_var], unown_y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "### Results with your implementation ###\n",
    "#########################################\n",
    "\n",
    "def eval_my_implementation(X_tr, y_tr, X_te, y_te):\n",
    "    # Now test your implementation of NearestCentroidClassifier\n",
    "    nearest_centroid = NearestCentroidClassifier()\n",
    "\n",
    "    # Fit on training dataset\n",
    "    nearest_centroid.fit(X_tr, y_tr)\n",
    "\n",
    "    # Make predictions on training and testing data\n",
    "    y_pred_tr = nearest_centroid.predict(X_tr)\n",
    "    y_pred_te = nearest_centroid.predict(X_te)\n",
    "\n",
    "    # Evaluate accuracies using your function compute_accuracy\n",
    "    acc_tr = compute_accuracy(y_tr, y_pred_tr)\n",
    "    acc_te = compute_accuracy(y_te, y_pred_te)\n",
    "\n",
    "    print(f'Your Results:')\n",
    "    print(f'--- Accuracy (train): {acc_tr}')\n",
    "    print(f'--- Accuracy (test): {acc_te}')\n",
    "\n",
    "    # Evaluate confusion matrix using your function compute_confusion_matrix\n",
    "    cm = compute_confusion_matrix(y_te, y_pred_te)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
    "    disp.plot(); \n",
    "\n",
    "# Call the function\n",
    "eval_my_implementation(unown_X_tr, unown_y_tr, unown_X_te, unown_y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Problem 3: kNN\n",
    "For the final problem of this homework, you will explore the k-nearest-neighbors algorithm using the Abalone dataset. You will use the `sklearn` implementation of kNN for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 3.1: Decision Boundaries\n",
    "\n",
    "\n",
    "#### Problem 3.1.a (4 points): Train/Test Split\n",
    "- Using the code in Problem 1 and Problem 2, to create a train/test split of the Abalone dataset containing two features `['Whole weight','Length']`. Use 75% of the data for training, and 25% of the data for testing. Set `shuffle=True` and be sure to use `random_state=hw1_seed`.\n",
    "- Fit a kNN classifier on this new training set, and plot the resulting decision boundary for values of `k = [1, 5, 10, 50]`.\n",
    "- Write a short description of what you see happen as you increase the value of `k`.\n",
    "\n",
    "Here are a few tips to help you get started.\n",
    "- In `sklearn`, you can create a kNN classifier with `k` neighbors via `knn = KNeighborsClassifier(n_neighbors=k)`.\n",
    "- You can then use `knn.fit(...)` and `knn.predict(...)` to fit the classifier and make predictions with it.\n",
    "- See [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for the corresponding documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# First define the abalone_y values for the abalone dataset\n",
    "abalone_y = abalone_clean['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a 75%/25% train/test split using 'Whole weight' and 'Length' features\n",
    "### YOUR CODE STARTS HERE ###\n",
    "abalone_X_tr2, abalone_X_te2, abalone_y_tr2, abalone_y_te2 = ...\n",
    "###  YOUR CODE ENDS HERE  ###\n",
    "abalone_X_tr2\n",
    "abalone_X_te2\n",
    "abalone_y_tr2\n",
    "abalone_y_te2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Problem 3.1.b (10 points): KNN Decision Boundary Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Fit a kNN classifier on this new training set, and plot the resulting decision boundary for values of `k = [1, 5, 10, 50]`.\n",
    "- Write a short description of what you see happen as you increase the value of `k`. \n",
    "- Hint: [this](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) function might be useful for the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundaries for the kNN classifiers for various values of k\n",
    "\n",
    "# Some keyword arguments for making nice looking plots.\n",
    "# Feel free to change grid_resolution to a higher number -- this results in better looking plots,\n",
    "# but may result in your code running more slowly.\n",
    "plot_kwargs = {'cmap': 'RdYlBu',\n",
    "               'response_method': 'predict',\n",
    "               'plot_method': 'pcolormesh',\n",
    "               'shading': 'auto',\n",
    "               'alpha': 0.5,\n",
    "               'grid_resolution': 100}\n",
    "\n",
    "\n",
    "# Create a figure with 2 rows and 2 columns\n",
    "figure, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "abalone_color = abalone_y_tr2.map(sex_to_color)\n",
    "\n",
    "k_vals = [1, 5, 10, 50]\n",
    "for i, k in enumerate(k_vals):\n",
    "    knn = ...\n",
    "    ...\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    axes[i//2, i%2].scatter(abalone_X_tr2['Whole weight'], abalone_X_tr2['Length'], c= ...\n",
    "    ...\n",
    "    axes[i//2, i%2].set_xlim((-0.5, 3))\n",
    "    axes[i//2, i%2].set_ylim((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 3.2 (15 points): Error Rates vs k \n",
    "\n",
    "Now, we will vary the value of $k$ and see what effect this has on our predictions.\n",
    "\n",
    "- Again, using only the given two features in the Abalone dataset, compute the error rate on both the training and testing data as a function of `k`. Do this for all values of `k = [1, 2, 5, 10, 50, 100, 110]`. You may use your own implementation of the accuracy from Problem 2.3, or the scikit-learn function `sklearn.metrics.accuracy_score`.\n",
    "- Plot the resulting error rate functions using a semi-log plot (i.e. the x-axis is on a logarithmic scale), with the training error in red and the validation error in green. This can be done using `axes[0].semlilogx(...)`. Use `matplotlib.pyplot` to do this, which is already imported for you as `plt`. Do not use any other plotting libraries, such as `pandas` or `seaborn`.\n",
    "- What value of `k` would you recommend, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a figure with only one subplot\n",
    "figure, axes = plt.subplots(1, figsize=(6, 6))\n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "...\n",
    "###  YOUR CODE ENDS HERE  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "### Statement of Collaboration (4 points)\n",
    "It is **mandatory** to include a Statement of Collaboration in each submission, with respect to the guidelines below. Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
    "\n",
    "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
    "programming assignments, in particular, I encourage the students to organize (perhaps using EdD) to\n",
    "discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start\n",
    "working on it. However, you should not discuss the specific solutions, and, as a guiding principle, you are not\n",
    "allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard,\n",
    "written notes, referring to EdD, etc.). Especially after you have started working on the assignment, try\n",
    "to restrict the discussion to EdD as much as possible, so that there is no doubt as to the extent of your\n",
    "collaboration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Please submit your hw1.ipynb by the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ics178_env)",
   "language": "python",
   "name": "ics178_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "otter": {
   "OK_FORMAT": false,
   "assignment_name": "hw1",
   "tests": {
    "Q1.1a Abalone Shape": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.1a Abalone Shape\"\npoints = None\n\n",
    "Q1.1b Abalone Features": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.1b Abalone Features\"\npoints = None\n\n",
    "Q1.1c Abalone Slicing": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.1c Abalone Slicing\"\npoints = None\n\n",
    "Q1.2 Feature Statistics": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.2 Feature Statistics\"\npoints = None\n\n",
    "Q1.3.a Abalone Features (Pandas)": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.a Abalone Features (Pandas)\"\npoints = None\n\n",
    "Q1.3.b Abalone Features by Column (Pandas)": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.b Abalone Features by Column (Pandas)\"\npoints = None\n\n",
    "Q1.3.c Abalone DataFrame Slicing (Pandas)": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.c Abalone DataFrame Slicing (Pandas)\"\npoints = None\n\n",
    "Q1.3.d Abalone DataFrame Subsetting (Pandas)": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.d Abalone DataFrame Subsetting (Pandas)\"\npoints = None\n\n",
    "Q1.3.e Abalone NaN Removal (Pandas)": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.e Abalone NaN Removal (Pandas)\"\npoints = None\n\n",
    "Q1.3.f Abalone Summary": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.f Abalone Summary\"\npoints = 1\n\n",
    "Q1.3.g Abalone Sample Statistics": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.g Abalone Sample Statistics\"\npoints = None\n\n",
    "Q1.3.h Object to Categorical": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.h Object to Categorical\"\npoints = None\n\n",
    "Q1.3.i Abalone Unique Classes": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"Q1.3.i Abalone Unique Classes\"\npoints = None\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
